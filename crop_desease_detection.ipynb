{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup"
      ],
      "metadata": {
        "id": "dsuDmDfCTERL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision\n",
        "!pip install -q ultralytics\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q opencv-python matplotlib pandas seaborn\n",
        "!pip install datasets huggingface-hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQh-tIg9S_85",
        "outputId": "5b21f0de-ff7d-4cbf-dcae-e907da2e01ca"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.13.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from huggingface_hub import hf_hub_download\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import random\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import yaml"
      ],
      "metadata": {
        "id": "G2Hg4I9QSjCT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xv5T34RSi_c",
        "outputId": "5c584bfe-6f0c-4ebb-8b23-41c773fbf891"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHkCat2iV2TZ",
        "outputId": "460f4351-0714-41d1-db72-858991684c55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May  9 09:20:41 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             45W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directories\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"weights\", exist_ok=True)\n",
        "os.makedirs(\"results\", exist_ok=True)"
      ],
      "metadata": {
        "id": "ZW6St6KRSi82"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Acquisition and Preprocessing"
      ],
      "metadata": {
        "id": "DBSs0iJWTTBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_pdt_dataset():\n",
        "    \"\"\"\n",
        "    Download a sample of the PDT dataset and place it in the correct location for Ultralytics.\n",
        "    \"\"\"\n",
        "    # The correct base directory for Ultralytics datasets\n",
        "    base_dir = \"/content/datasets\"\n",
        "    dataset_dir = os.path.join(base_dir, \"PDT\")\n",
        "\n",
        "    # Create directories\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        os.makedirs(os.path.join(dataset_dir, \"images\", split), exist_ok=True)\n",
        "        os.makedirs(os.path.join(dataset_dir, \"labels\", split), exist_ok=True)\n",
        "\n",
        "    print(\"Creating placeholder data for demonstration...\")\n",
        "\n",
        "    # Create placeholder images and labels\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        num_images = 20 if split == \"train\" else 5\n",
        "        for i in range(num_images):\n",
        "            # Create a black image with a white rectangle as an \"unhealthy\" area\n",
        "            img = np.zeros((640, 640, 3), dtype=np.uint8)\n",
        "            x1, y1 = random.randint(100, 400), random.randint(100, 400)\n",
        "            x2, y2 = x1 + random.randint(50, 200), y1 + random.randint(50, 200)\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 255, 255), -1)\n",
        "\n",
        "            # Save image\n",
        "            img_path = os.path.join(dataset_dir, \"images\", split, f\"placeholder_{i}.jpg\")\n",
        "            cv2.imwrite(img_path, img)\n",
        "\n",
        "            # Save label (YOLO format: class x_center y_center width height)\n",
        "            label_path = os.path.join(dataset_dir, \"labels\", split, f\"placeholder_{i}.txt\")\n",
        "\n",
        "            # Calculate YOLO format coordinates\n",
        "            img_width, img_height = 640, 640\n",
        "            x_center = (x1 + x2) / 2 / img_width\n",
        "            y_center = (y1 + y2) / 2 / img_height\n",
        "            width = (x2 - x1) / img_width\n",
        "            height = (y2 - y1) / img_height\n",
        "\n",
        "            with open(label_path, \"w\") as f:\n",
        "                f.write(f\"0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
        "\n",
        "    print(f\"Created placeholder data in {dataset_dir}\")\n",
        "    return dataset_dir"
      ],
      "metadata": {
        "id": "o3-qOuBgWPlV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_config(dataset_path):\n",
        "    \"\"\"Create YAML config file for YOLO training\"\"\"\n",
        "    data_yaml = {\n",
        "        'path': dataset_path,  # Use the absolute path\n",
        "        'train': 'images/train',  # Relative to path\n",
        "        'val': 'images/val',      # Relative to path\n",
        "        'names': {0: 'unhealthy'},\n",
        "        'nc': 1  # number of classes\n",
        "    }\n",
        "\n",
        "    config_path = os.path.join(dataset_path, \"data.yaml\")\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(data_yaml, f)\n",
        "\n",
        "    print(f\"Created data configuration at {config_path}\")\n",
        "    return config_path"
      ],
      "metadata": {
        "id": "vGGngAiMWPiu"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. YOLO-DP Model Architecture"
      ],
      "metadata": {
        "id": "MbvAx-BEWUpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GhostConv(torch.nn.Module):\n",
        "    \"\"\"Ghost Convolution from GhostNet\"\"\"\n",
        "    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):\n",
        "        super(GhostConv, self).__init__()\n",
        "        c_ = c2 // 2  # hidden channels\n",
        "        self.cv1 = torch.nn.Conv2d(c1, c_, k, s, k//2, groups=g, bias=False)\n",
        "        self.cv2 = torch.nn.Conv2d(c_, c_, 3, 1, 1, groups=c_, bias=False)\n",
        "        self.bn = torch.nn.BatchNorm2d(c2)\n",
        "        self.act = torch.nn.SiLU() if act else torch.nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.cv1(x)\n",
        "        return self.act(self.bn(torch.cat((y, self.cv2(y)), 1)))"
      ],
      "metadata": {
        "id": "vr-kF-JMWPgH"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaptiveLargeScaleSelectiveKernel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Adaptive Large Scale Selective Kernel as described in YOLO-DP paper\n",
        "    For capturing the location information of dense, small target pest-infested trees\n",
        "    \"\"\"\n",
        "    def __init__(self, c1, c2):\n",
        "        super(AdaptiveLargeScaleSelectiveKernel, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(c1, c2, kernel_size=5, padding=2)  # Shallow range\n",
        "        self.conv2 = torch.nn.Conv2d(c2, c2, kernel_size=7, padding=9)  # Deep range\n",
        "\n",
        "        # GhostConv for feature selection\n",
        "        self.ghost1 = GhostConv(c2, c2, k=1)\n",
        "        self.ghost2 = GhostConv(c2, c2, k=1)\n",
        "\n",
        "        # Spatial attention\n",
        "        self.spatial_attn = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(2, 1, kernel_size=7, padding=3),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.final_conv = torch.nn.Conv2d(c2, c2, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract shallow and deep range info\n",
        "        m1 = self.conv1(x)\n",
        "        m2 = self.conv2(m1)\n",
        "\n",
        "        # Feature selection\n",
        "        m3 = self.ghost1(m1)\n",
        "        m4 = self.ghost2(m2)\n",
        "\n",
        "        # Spatial attention\n",
        "        avg_pool = torch.mean(torch.cat([m3, m4], dim=1), dim=1, keepdim=True)\n",
        "        max_pool, _ = torch.max(torch.cat([m3, m4], dim=1), dim=1, keepdim=True)\n",
        "        spatial_attn = self.spatial_attn(torch.cat([avg_pool, max_pool], dim=1))\n",
        "\n",
        "        # Apply attention and combine\n",
        "        m12 = m3 * spatial_attn\n",
        "        m13 = m4 * spatial_attn\n",
        "        output = self.final_conv(m12 + m13)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "RG-NtZ2HWPdC"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a simplified implementation of YOLO-DP\n",
        "# In practice, i would modify the YOLOv5 code directly\n",
        "# or use the Ultralytics YOLO framework with custom modules\n",
        "\n",
        "def create_yolo_dp_model(pretrained=False):\n",
        "    \"\"\"\n",
        "    Create a YOLO-DP model.\n",
        "    For simplicity, we'll use Ultralytics YOLOv5 as the base and\n",
        "    describe how you would customize it to implement YOLO-DP\n",
        "    \"\"\"\n",
        "    # In a real implementation, you would modify the YOLOv5 code directly\n",
        "    # Here we're using YOLOv5s as a base and explaining the modifications\n",
        "\n",
        "    print(\"Creating YOLO-DP model based on YOLOv5s...\")\n",
        "\n",
        "    # For actual implementation:\n",
        "    # 1. Clone the YOLOv5 repository\n",
        "    # 2. Modify models/common.py to add GhostConv and AdaptiveLargeScaleSelectiveKernel\n",
        "    # 3. Modify models/yolo.py to use these modules in the backbone and neck\n",
        "    # 4. Implement decoupled detection heads\n",
        "\n",
        "    # For this example, we'll use YOLOv5s from Ultralytics\n",
        "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=pretrained)\n",
        "\n",
        "    print(\"YOLO-DP model created (simplified version)!\")\n",
        "\n",
        "    # Add a note about the actual implementation\n",
        "    print(\"\\nNote: This is a simplified version of YOLO-DP.\")\n",
        "    print(\"For a full implementation, you would need to modify the YOLOv5 architecture\")\n",
        "    print(\"by integrating the Adaptive Large Scale Selective Kernel and GhostConv modules\")\n",
        "    print(\"as described in the paper.\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "KSeJUctbXngL"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = create_yolo_dp_model(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4fOCJqFXr2C",
        "outputId": "34d12ece-7a1c-4197-e358-1db896890ac8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2025-5-9 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\n",
            "Fusing layers... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating YOLO-DP model based on YOLOv5s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO-DP model created (simplified version)!\n",
            "\n",
            "Note: This is a simplified version of YOLO-DP.\n",
            "For a full implementation, you would need to modify the YOLOv5 architecture\n",
            "by integrating the Adaptive Large Scale Selective Kernel and GhostConv modules\n",
            "as described in the paper.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training Pipeline"
      ],
      "metadata": {
        "id": "vCOAkKyTX0sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(data_config, epochs=100, batch_size=16, img_size=640):\n",
        "    \"\"\"\n",
        "    Train the YOLO-DP model.\n",
        "    Using YOLOv5 training pipeline with custom configuration.\n",
        "    \"\"\"\n",
        "    print(f\"Starting training for {epochs} epochs...\")\n",
        "\n",
        "    # Use YOLOv5s as the base model (newer versions might allow yolov5su.pt)\n",
        "    !yolo train \\\n",
        "        model=yolov5s.pt \\\n",
        "        data={data_config} \\\n",
        "        epochs={epochs} \\\n",
        "        batch={batch_size} \\\n",
        "        imgsz={img_size} \\\n",
        "        patience=100 \\\n",
        "        project=runs/train \\\n",
        "        name=YOLO-DP \\\n",
        "        exist_ok=True\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    return \"runs/train/YOLO-DP/weights/best.pt\""
      ],
      "metadata": {
        "id": "jWxxVIgYYHGQ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the pipeline\n",
        "dataset_path = download_pdt_dataset()\n",
        "data_config = prepare_data_config(dataset_path)\n",
        "best_weights = train_model(data_config, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_cXVYQjYHDY",
        "outputId": "b668b403-8cd3-4909-e723-29607ae1b07e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating placeholder data for demonstration...\n",
            "Created placeholder data in /content/datasets/PDT\n",
            "Created data configuration at /content/datasets/PDT/data.yaml\n",
            "Starting training for 10 epochs...\n",
            "PRO TIP ðŸ’¡ Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
            "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
            "\n",
            "Ultralytics 8.3.129 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/datasets/PDT/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov5s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=YOLO-DP, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/train/YOLO-DP, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 14.8MB/s]\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      3520  ultralytics.nn.modules.conv.Conv             [3, 32, 6, 2, 2]              \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     18816  ultralytics.nn.modules.block.C3              [64, 64, 1]                   \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    115712  ultralytics.nn.modules.block.C3              [128, 128, 2]                 \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  3    625152  ultralytics.nn.modules.block.C3              [256, 256, 3]                 \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1]                 \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    361984  ultralytics.nn.modules.block.C3              [512, 256, 1, False]          \n",
            " 14                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
            " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 17                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n",
            " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 20                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n",
            " 21                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 23                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1, False]          \n",
            " 24        [17, 20, 23]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
            "YOLOv5s summary: 153 layers, 9,122,579 parameters, 9,122,563 gradients, 24.0 GFLOPs\n",
            "\n",
            "Transferred 421/427 items from pretrained weights\n",
            "Freezing layer 'model.24.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n",
            "100% 5.35M/5.35M [00:00<00:00, 64.1MB/s]\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 373.7Â±135.9 MB/s, size: 8.3 KB)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/PDT/labels/train... 20 images, 0 backgrounds, 0 corrupt: 100% 20/20 [00:00<00:00, 1534.74it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/PDT/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 240.2Â±100.0 MB/s, size: 8.3 KB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/PDT/labels/val... 5 images, 0 backgrounds, 0 corrupt: 100% 5/5 [00:00<00:00, 1015.91it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/PDT/labels/val.cache\n",
            "Plotting labels to runs/train/YOLO-DP/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 69 weight(decay=0.0), 76 weight(decay=0.0005), 75 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/YOLO-DP\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/10      3.85G      1.221      4.096      1.011          4        640: 100% 2/2 [00:01<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.09it/s]\n",
            "                   all          5          5    0.00333          1    0.00791    0.00451\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/10      3.87G      1.177      4.258      1.054          4        640: 100% 2/2 [00:00<00:00,  9.69it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00, 12.39it/s]\n",
            "                   all          5          5    0.00333          1    0.00815    0.00482\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/10      3.88G      1.055      3.782      0.961          4        640: 100% 2/2 [00:00<00:00, 10.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00, 23.20it/s]\n",
            "                   all          5          5    0.00333          1    0.00873    0.00532\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/10      3.89G      1.221      3.595      1.167          4        640: 100% 2/2 [00:00<00:00, 11.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00, 23.66it/s]\n",
            "                   all          5          5    0.00333          1     0.0097    0.00619\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/10      3.91G      1.023      3.802      1.028          4        640: 100% 2/2 [00:00<00:00,  6.47it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00, 21.22it/s]\n",
            "                   all          5          5     0.0103          1      0.787      0.527\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/10      3.96G     0.9078       2.88     0.9237          4        640: 100% 2/2 [00:00<00:00, 10.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00, 20.66it/s]\n",
            "                   all          5          5          1      0.652      0.938      0.855\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/10      3.99G     0.5255      2.519     0.7997          4        640: 100% 2/2 [00:00<00:00,  9.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00, 12.17it/s]\n",
            "                   all          5          5      0.584        0.8      0.747       0.69\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/10      4.02G     0.4781      1.643      0.769          4        640: 100% 2/2 [00:00<00:00,  8.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00, 19.87it/s]\n",
            "                   all          5          5      0.973          1      0.995      0.899\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/10      4.05G      0.318       1.22     0.8149          4        640: 100% 2/2 [00:00<00:00, 10.46it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00, 18.03it/s]\n",
            "                   all          5          5      0.976          1      0.995      0.866\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/10      4.09G     0.4239       1.19     0.8215          4        640: 100% 2/2 [00:00<00:00, 10.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00, 22.80it/s]\n",
            "                   all          5          5      0.956          1      0.995      0.867\n",
            "\n",
            "10 epochs completed in 0.002 hours.\n",
            "Optimizer stripped from runs/train/YOLO-DP/weights/last.pt, 18.5MB\n",
            "Optimizer stripped from runs/train/YOLO-DP/weights/best.pt, 18.5MB\n",
            "\n",
            "Validating runs/train/YOLO-DP/weights/best.pt...\n",
            "Ultralytics 8.3.129 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "YOLOv5s summary (fused): 84 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00, 26.49it/s]\n",
            "                   all          5          5      0.973          1      0.995      0.899\n",
            "Speed: 0.2ms preprocess, 1.7ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/train/YOLO-DP\u001b[0m\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Evaluation"
      ],
      "metadata": {
        "id": "7MBE2-EZYRKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model_path, data_config, batch_size=16, img_size=640):\n",
        "    \"\"\"Evaluate the trained model.\"\"\"\n",
        "    print(f\"Evaluating model from {model_path}...\")\n",
        "\n",
        "    # Using YOLOv5's val.py script for evaluation\n",
        "    !yolo val \\\n",
        "        model={model_path} \\\n",
        "        data={data_config} \\\n",
        "        batch={batch_size} \\\n",
        "        imgsz={img_size} \\\n",
        "        project=runs/val \\\n",
        "        name=YOLO-DP-eval \\\n",
        "        exist_ok=True\n",
        "\n",
        "    print(\"Evaluation completed!\")"
      ],
      "metadata": {
        "id": "VYt6US4bYHAy"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(best_weights, data_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Sg6MQ74YG97",
        "outputId": "24768b0f-0be4-462e-a8dc-b5ed70c143de"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model from runs/train/YOLO-DP/weights/best.pt...\n",
            "Ultralytics 8.3.129 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "YOLOv5s summary (fused): 84 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 506.7Â±110.0 MB/s, size: 8.3 KB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/PDT/labels/val.cache... 5 images, 0 backgrounds, 0 corrupt: 100% 5/5 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  3.41it/s]\n",
            "                   all          5          5      0.973          1      0.995      0.899\n",
            "Speed: 1.2ms preprocess, 11.4ms inference, 0.0ms loss, 27.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/val/YOLO-DP-eval\u001b[0m\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n",
            "Evaluation completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualization and Inference"
      ],
      "metadata": {
        "id": "v_7G35wyYbNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_predictions(model_path, img_path, conf_threshold=0.25):\n",
        "    \"\"\"Visualize model predictions on a single image.\"\"\"\n",
        "    # Load model\n",
        "    model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path)\n",
        "    model.conf = conf_threshold\n",
        "\n",
        "    # Make prediction\n",
        "    results = model(img_path)\n",
        "\n",
        "    # Visualize\n",
        "    results.show()\n",
        "\n",
        "    # Save results\n",
        "    output_path = f\"results/prediction_{Path(img_path).stem}.jpg\"\n",
        "    results.save(save_dir=\"results\")\n",
        "    print(f\"Saved prediction to {output_path}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "a99t5_wrYG6R"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_on_folder(weights_path, folder_path, conf_threshold=0.25, save_dir=\"results\"):\n",
        "    \"\"\"\n",
        "    Run inference on all images in a specified folder.\n",
        "\n",
        "    Args:\n",
        "        weights_path: Path to the trained model weights\n",
        "        folder_path: Path to the folder containing images to run inference on\n",
        "        conf_threshold: Confidence threshold for detections (0-1)\n",
        "        save_dir: Directory to save results\n",
        "\n",
        "    Returns:\n",
        "        output_path: Path to the saved results\n",
        "    \"\"\"\n",
        "    # Check if weights exist\n",
        "    if not os.path.exists(weights_path):\n",
        "        print(f\"Error: Weights file not found at {weights_path}\")\n",
        "        return None\n",
        "\n",
        "    # Check if folder exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"Error: Folder not found at {folder_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Running inference on folder: {folder_path}\")\n",
        "    print(f\"Using model weights: {weights_path}\")\n",
        "    print(f\"Confidence threshold: {conf_threshold}\")\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Run inference using YOLO command\n",
        "    output_path = os.path.join(save_dir, \"pdt_predictions\")\n",
        "\n",
        "    # Use Ultralytics YOLO for inference\n",
        "    !yolo predict model={weights_path} source={folder_path} conf={conf_threshold} project={save_dir} name=pdt_predictions save=True\n",
        "\n",
        "    print(f\"Inference complete! Results saved to {output_path}\")\n",
        "\n",
        "    # Count the number of prediction images\n",
        "    try:\n",
        "        num_predictions = len(list(Path(output_path).glob(\"*.jpg\")))\n",
        "        print(f\"Generated {num_predictions} prediction images\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not count prediction images: {e}\")\n",
        "\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "YJd5ar1PYiv2"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your trained weights\n",
        "best_weights = \"runs/train/YOLO-DP/weights/best.pt\"\n",
        "\n",
        "# Path to folder with images for inference\n",
        "test_folder = \"path/to/test/folder\"  # Replace with your actual test folder path\n",
        "\n",
        "# Run inference\n",
        "results_path = inference_on_folder(best_weights, test_folder, conf_threshold=0.25)\n",
        "\n",
        "# Print results path\n",
        "print(f\"Inference results saved to: {results_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN6LGiShYmzf",
        "outputId": "54750e65-68b1-4f36-f0bf-0f0ccab99b08"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Folder not found at path/to/test/folder\n",
            "Inference results saved to: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Comparison with Other Models"
      ],
      "metadata": {
        "id": "0VZPVyUsYprE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models():\n",
        "    \"\"\"\n",
        "    Compare the performance of YOLO-DP with other models as shown in the paper.\n",
        "    This is for reference; actual implementation would depend on the models.\n",
        "    \"\"\"\n",
        "    # Create a table of results similar to Table 6 in the paper\n",
        "    models = [\n",
        "        \"YOLO-DP\", \"YOLOv3\", \"YOLOv4s\", \"YOLOv5s\", \"YOLOv7\", \"YOLOv8s\"\n",
        "    ]\n",
        "\n",
        "    pdt_results = {\n",
        "        \"YOLO-DP\": {\"P\": 90.2, \"R\": 88.0, \"mAP@.5\": 94.5, \"mAP@.5:.95\": 67.5, \"F1\": 0.89, \"GFLOPs\": 11.7, \"FPS\": 109},\n",
        "        \"YOLOv3\": {\"P\": 88.5, \"R\": 88.1, \"mAP@.5\": 93.4, \"mAP@.5:.95\": 65.7, \"F1\": 0.88, \"GFLOPs\": 155.3, \"FPS\": 41},\n",
        "        \"YOLOv4s\": {\"P\": 88.8, \"R\": 88.2, \"mAP@.5\": 94.7, \"mAP@.5:.95\": 66.1, \"F1\": 0.88, \"GFLOPs\": 20.8, \"FPS\": 51},\n",
        "        \"YOLOv5s\": {\"P\": 88.9, \"R\": 88.5, \"mAP@.5\": 94.2, \"mAP@.5:.95\": 67.0, \"F1\": 0.89, \"GFLOPs\": 16.0, \"FPS\": 93},\n",
        "        \"YOLOv7\": {\"P\": 87.4, \"R\": 82.6, \"mAP@.5\": 90.1, \"mAP@.5:.95\": 55.5, \"F1\": 0.85, \"GFLOPs\": 105.1, \"FPS\": 32},\n",
        "        \"YOLOv8s\": {\"P\": 88.7, \"R\": 87.5, \"mAP@.5\": 94.0, \"mAP@.5:.95\": 67.9, \"F1\": 0.88, \"GFLOPs\": 28.6, \"FPS\": 60}\n",
        "    }\n",
        "\n",
        "    # Print the comparison table\n",
        "    print(\"Model Comparison on PDT Dataset:\")\n",
        "    print(f\"{'Model':<10} {'Precision':<10} {'Recall':<10} {'mAP@.5':<10} {'mAP@.5:.95':<12} {'F1':<8} {'GFLOPs':<8} {'FPS':<6}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for model in models:\n",
        "        result = pdt_results[model]\n",
        "        print(f\"{model:<10} {result['P']:<10.1f} {result['R']:<10.1f} {result['mAP@.5']:<10.1f} {result['mAP@.5:.95']:<12.1f} {result['F1']:<8.2f} {result['GFLOPs']:<8.1f} {result['FPS']:<6.0f}\")\n",
        "\n",
        "# Show the performance comparison\n",
        "compare_models()"
      ],
      "metadata": {
        "id": "cDfEda34YoOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Complete Pipeline Example"
      ],
      "metadata": {
        "id": "7B6XIHDnYwFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def full_pipeline_example():\n",
        "    \"\"\"\n",
        "    Example of running the complete pipeline.\n",
        "    \"\"\"\n",
        "    print(\"Complete Pipeline Example:\")\n",
        "    print(\"1. Download PDT dataset\")\n",
        "    dataset_path = download_pdt_dataset()\n",
        "\n",
        "    print(\"\\n2. Prepare data configuration\")\n",
        "    data_config = prepare_data_config(dataset_path)\n",
        "\n",
        "    print(\"\\n3. Create YOLO-DP model\")\n",
        "    model = create_yolo_dp_model(pretrained=True)\n",
        "\n",
        "    print(\"\\n4. Train the model (skipping for this example)\")\n",
        "    # best_weights = train_model(model, data_config, epochs=300, batch_size=16)\n",
        "\n",
        "    print(\"\\n5. Evaluate the model (skipping for this example)\")\n",
        "    # evaluate_model(best_weights, data_config)\n",
        "\n",
        "    print(\"\\n6. Run inference (skipping for this example)\")\n",
        "    # visualize_predictions(best_weights, \"sample_image.jpg\")\n",
        "\n",
        "    print(\"\\nComplete pipeline example finished!\")\n",
        "    print(\"In a real implementation, you would:\")\n",
        "    print(\"1. Download the full PDT dataset from HuggingFace\")\n",
        "    print(\"2. Implement the YOLO-DP architecture by modifying YOLOv5\")\n",
        "    print(\"3. Train on a GPU for 300 epochs\")\n",
        "    print(\"4. Evaluate and compare with other models\")\n",
        "    print(\"5. Use the model for real-world tree disease detection\")"
      ],
      "metadata": {
        "id": "1NcWA1JFYoIT"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_pipeline_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDpiEwAsYoFL",
        "outputId": "9b9c9628-77fc-4d62-e254-bd1e20a6f802"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete Pipeline Example:\n",
            "1. Download PDT dataset\n",
            "Creating placeholder data for demonstration...\n",
            "Created placeholder data in /content/datasets/PDT\n",
            "\n",
            "2. Prepare data configuration\n",
            "Created data configuration at /content/datasets/PDT/data.yaml\n",
            "\n",
            "3. Create YOLO-DP model\n",
            "Creating YOLO-DP model based on YOLOv5s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2025-5-9 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO-DP model created (simplified version)!\n",
            "\n",
            "Note: This is a simplified version of YOLO-DP.\n",
            "For a full implementation, you would need to modify the YOLOv5 architecture\n",
            "by integrating the Adaptive Large Scale Selective Kernel and GhostConv modules\n",
            "as described in the paper.\n",
            "\n",
            "4. Train the model (skipping for this example)\n",
            "\n",
            "5. Evaluate the model (skipping for this example)\n",
            "\n",
            "6. Run inference (skipping for this example)\n",
            "\n",
            "Complete pipeline example finished!\n",
            "In a real implementation, you would:\n",
            "1. Download the full PDT dataset from HuggingFace\n",
            "2. Implement the YOLO-DP architecture by modifying YOLOv5\n",
            "3. Train on a GPU for 300 epochs\n",
            "4. Evaluate and compare with other models\n",
            "5. Use the model for real-world tree disease detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Additional Resources"
      ],
      "metadata": {
        "id": "kyItgNbsY4Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAdditional Resources:\")\n",
        "print(\"1. PDT Dataset: https://huggingface.co/datasets/qwer0213/PDT_dataset\")\n",
        "print(\"2. Original Paper: https://ar5iv.labs.arxiv.org/html/2409.15679\")\n",
        "print(\"3. YOLO-DP Code: https://github.com/RuiXing123/PDT_CWC_YOLO-DP\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnrpmOYRYoCX",
        "outputId": "f9b9f4f3-bc79-4173-9791-ef3c0334aefd"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Additional Resources:\n",
            "1. PDT Dataset: https://huggingface.co/datasets/qwer0213/PDT_dataset\n",
            "2. Original Paper: https://ar5iv.labs.arxiv.org/html/2409.15679\n",
            "3. YOLO-DP Code: https://github.com/RuiXing123/PDT_CWC_YOLO-DP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "McPBO98gSi5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7GROAqGeSi3H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}